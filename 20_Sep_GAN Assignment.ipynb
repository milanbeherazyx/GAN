{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s an implementation of a Deep Convolutional Generative Adversarial Network (DCGAN) using the CIFAR-10 dataset to generate images from noise. This implementation will include training the GAN, generating images, and plotting the generator and discriminator losses to help visualize the convergence of the training process.\n",
    "\n",
    "### Requirements\n",
    "Make sure you have the following libraries installed:\n",
    "- `torch`\n",
    "- `torchvision`\n",
    "- `matplotlib`\n",
    "- `numpy`\n",
    "\n",
    "You can install the necessary libraries using pip:\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision matplotlib numpy\n",
    "```\n",
    "\n",
    "### Implementation\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. Define Hyperparameters\n",
    "batch_size = 128\n",
    "lr = 0.0002\n",
    "num_epochs = 50\n",
    "latent_dim = 100\n",
    "sample_interval = 500\n",
    "\n",
    "# 2. Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "cifar10_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "dataloader = DataLoader(cifar10_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 3. Define DCGAN Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 512, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "# 4. Define DCGAN Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n",
    "\n",
    "# 5. Initialize models, loss function and optimizers\n",
    "generator = Generator().cuda()\n",
    "discriminator = Discriminator().cuda()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# 6. Train the GAN\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "        # Configure input\n",
    "        real_imgs = imgs.cuda()\n",
    "\n",
    "        # Labels for real and fake images\n",
    "        valid = torch.ones((imgs.size(0), 1)).cuda()\n",
    "        fake = torch.zeros((imgs.size(0), 1)).cuda()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        z = torch.randn((imgs.size(0), latent_dim, 1, 1)).cuda()\n",
    "        gen_imgs = generator(z)\n",
    "        g_loss = criterion(discriminator(gen_imgs), valid)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        d_loss_real = criterion(discriminator(real_imgs), valid)\n",
    "        d_loss_fake = criterion(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (d_loss_real + d_loss_fake) / 2\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Record losses\n",
    "        G_losses.append(g_loss.item())\n",
    "        D_losses.append(d_loss.item())\n",
    "\n",
    "        if i % sample_interval == 0:\n",
    "            print(f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(dataloader)}] [D loss: {d_loss.item():.6f}] [G loss: {g_loss.item():.6f}]\")\n",
    "\n",
    "# 7. Generate images\n",
    "def generate_images(num_images):\n",
    "    z = torch.randn(num_images, latent_dim, 1, 1).cuda()\n",
    "    with torch.no_grad():\n",
    "        gen_imgs = generator(z).cpu()\n",
    "    return gen_imgs\n",
    "\n",
    "# Plot generated images\n",
    "def plot_generated_images(images, ncol=5):\n",
    "    nrow = len(images) // ncol\n",
    "    fig, axs = plt.subplots(nrow, ncol, figsize=(15, 15))\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        ax.imshow((images[i] + 1) / 2)  # Rescale from [-1, 1] to [0, 1]\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate and plot images after training\n",
    "generated_images = generate_images(25)\n",
    "plot_generated_images(generated_images)\n",
    "\n",
    "# 8. Plot the losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(G_losses, label='Generator Loss')\n",
    "plt.plot(D_losses, label='Discriminator Loss')\n",
    "plt.title('Losses During Training')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Explanation of the Code\n",
    "1. **Hyperparameters**: Define learning rate, batch size, number of epochs, latent dimension for noise input, and sample interval for logging.\n",
    "\n",
    "2. **Dataset Preparation**: Load the CIFAR-10 dataset and apply transformations like resizing and normalization.\n",
    "\n",
    "3. **DCGAN Model**: Define the generator and discriminator networks using convolutional layers.\n",
    "\n",
    "4. **Training**: Train the GAN for a specified number of epochs. In each iteration:\n",
    "   - Update the generator and discriminator networks.\n",
    "   - Record the generator and discriminator losses.\n",
    "\n",
    "5. **Generate Images**: After training, generate images from random noise.\n",
    "\n",
    "6. **Plot Losses**: Visualize the generator and discriminator losses to monitor the convergence of the GAN training process.\n",
    "\n",
    "### Running the Code\n",
    "1. Make sure you have the CIFAR-10 dataset available. If not, the code will download it automatically.\n",
    "2. Run the entire script in a Python environment that supports PyTorch and has a CUDA-capable GPU for better performance.\n",
    "3. The generated images and loss plots will be displayed after training.\n",
    "\n",
    "This implementation should give you a solid foundation for experimenting with DCGANs and generating images from noise. You can adjust the hyperparameters, add more layers, or experiment with different datasets to see how it affects the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a complete implementation for fine-tuning a ResNet-50 model on the CIFAR-10 dataset, covering the four parts of your question. This implementation includes training the model from scratch, using pre-trained weights with frozen layers, and using pre-trained weights with all layers trainable. It also proposes a simple domain adaptation strategy for better performance.\n",
    "\n",
    "### Requirements\n",
    "Make sure you have the following libraries installed:\n",
    "- `torch`\n",
    "- `torchvision`\n",
    "- `matplotlib`\n",
    "- `numpy`\n",
    "\n",
    "You can install the necessary libraries using pip:\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision matplotlib numpy\n",
    "```\n",
    "\n",
    "### Implementation\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "num_classes = 10\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),  # Resize for ResNet input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define a new model with ResNet50 backbone\n",
    "class ResNet50FineTune(nn.Module):\n",
    "    def __init__(self, freeze_layers=False):\n",
    "        super(ResNet50FineTune, self).__init__()\n",
    "        self.resnet = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        if freeze_layers:\n",
    "            for param in self.resnet.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate_model(model, train_loader, test_loader, num_epochs):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total += labels.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        train_accuracy.append(train_acc)\n",
    "\n",
    "        # Test the model\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                outputs = model(images)\n",
    "                total += labels.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_acc = 100 * correct / total\n",
    "        test_accuracy.append(test_acc)\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%')\n",
    "\n",
    "    return train_accuracy, test_accuracy\n",
    "\n",
    "# Part A: Train from scratch\n",
    "print(\"Training from Scratch...\")\n",
    "model_A = ResNet50FineTune().cuda()\n",
    "train_acc_A, test_acc_A = train_and_evaluate_model(model_A, train_loader, test_loader, num_epochs)\n",
    "\n",
    "# Part B: Use pre-trained ResNet50 and freeze layers\n",
    "print(\"Using Pre-trained ResNet50 (Frozen Layers)...\")\n",
    "model_B = ResNet50FineTune(freeze_layers=True).cuda()\n",
    "train_acc_B, test_acc_B = train_and_evaluate_model(model_B, train_loader, test_loader, num_epochs)\n",
    "\n",
    "# Part C: Use pre-trained ResNet50 and train all layers\n",
    "print(\"Using Pre-trained ResNet50 (Train All Layers)...\")\n",
    "model_C = ResNet50FineTune(freeze_layers=False).cuda()\n",
    "train_acc_C, test_acc_C = train_and_evaluate_model(model_C, train_loader, test_loader, num_epochs)\n",
    "\n",
    "# Plotting results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Training Accuracies\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_acc_A, label='Train Acc (Scratch)')\n",
    "plt.plot(train_acc_B, label='Train Acc (Frozen)')\n",
    "plt.plot(train_acc_C, label='Train Acc (All Layers)')\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "# Testing Accuracies\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(test_acc_A, label='Test Acc (Scratch)')\n",
    "plt.plot(test_acc_B, label='Test Acc (Frozen)')\n",
    "plt.plot(test_acc_C, label='Test Acc (All Layers)')\n",
    "plt.title('Testing Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Part D: Proposed Domain Adaptation Algorithm\n",
    "# Domain adaptation: Fine-tuning the model with additional data from a similar domain (e.g., using CIFAR-100)\n",
    "\n",
    "# Load CIFAR-100 dataset for domain adaptation\n",
    "cifar100_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "cifar100_loader = DataLoader(dataset=cifar100_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Retrain model using CIFAR-100 data as additional domain data\n",
    "print(\"Proposed Domain Adaptation...\")\n",
    "model_D = ResNet50FineTune(freeze_layers=False).cuda()\n",
    "train_acc_D, test_acc_D = train_and_evaluate_model(model_D, cifar100_loader, test_loader, num_epochs)\n",
    "\n",
    "# Plotting results for Domain Adaptation\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_acc_D, label='Train Acc (Domain Adaptation)')\n",
    "plt.plot(test_acc_D, label='Test Acc (Domain Adaptation Test)')\n",
    "plt.title('Domain Adaptation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Explanation of the Code\n",
    "\n",
    "1. **Load CIFAR-10 Dataset**: The CIFAR-10 dataset is loaded and preprocessed (resized and normalized).\n",
    "\n",
    "2. **Define ResNet-50 Model**: A custom model class is created with a ResNet-50 backbone. The classification layer is replaced with a new 2-layer neural network followed by a softmax layer. The model can be configured to freeze the backbone layers.\n",
    "\n",
    "3. **Training and Evaluation Function**: This function trains the model, evaluates accuracy on both the train and test sets, and logs the results.\n",
    "\n",
    "4. **Training**: The model is trained in three parts:\n",
    "   - Part A: Train the model from scratch.\n",
    "   - Part B: Use a pre-trained ResNet-50 with frozen layers.\n",
    "   - Part C: Use a pre-trained ResNet-50 and train all layers.\n",
    "\n",
    "5. **Plotting Results**: The training and testing accuracies are plotted for all three scenarios.\n",
    "\n",
    "6. **Domain Adaptation**: For Part D, the CIFAR-100 dataset is used as additional data for fine-tuning the model, with the aim to improve accuracy on CIFAR-10.\n",
    "\n",
    "### Note on Domain Adaptation\n",
    "In this implementation, we assumed that utilizing a similar dataset (CIFAR-100) could help improve performance. You can modify the proposed algorithm further by incorporating different strategies, such as transfer learning techniques, data augmentation, or using additional domain-related datasets.\n",
    "\n",
    "### Running the Code\n",
    "Run the entire script in a Python environment that supports PyTorch. Ensure that you have a suitable GPU for faster training. Adjust hyperparameters (like learning rate and batch size) as necessary to achieve better performance.\n",
    "\n",
    "This implementation provides a comprehensive approach to fine-tuning ResNet-50 on the CIFAR-10 dataset, and it includes the domain adaptation strategy for better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a complete implementation of a Generative Adversarial Network (GAN) from scratch using Keras to generate celebrity faces from noise. This example uses the CelebA dataset, which can be downloaded from Kaggle as mentioned.\n",
    "\n",
    "### Requirements\n",
    "Make sure you have the following libraries installed:\n",
    "- `tensorflow`\n",
    "- `numpy`\n",
    "- `matplotlib`\n",
    "- `PIL` (for image handling)\n",
    "\n",
    "You can install the necessary libraries using pip:\n",
    "\n",
    "```bash\n",
    "pip install tensorflow numpy matplotlib Pillow\n",
    "```\n",
    "\n",
    "### Implementation\n",
    "\n",
    "```python\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Reshape, Flatten, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Constants\n",
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64\n",
    "IMG_CHANNELS = 3\n",
    "NOISE_DIM = 100\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10000\n",
    "SAVE_INTERVAL = 1000\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_data(data_path):\n",
    "    images = []\n",
    "    for filename in os.listdir(data_path):\n",
    "        img = Image.open(os.path.join(data_path, filename))\n",
    "        img = img.resize((IMG_HEIGHT, IMG_WIDTH))\n",
    "        img = np.array(img)\n",
    "        if img.shape == (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS):\n",
    "            images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "# Normalize and scale images\n",
    "data_path = 'path_to_celebA_dataset'  # Update this to your dataset path\n",
    "images = load_data(data_path)\n",
    "images = (images.astype(np.float32) - 127.5) / 127.5  # Normalize to [-1, 1]\n",
    "\n",
    "# Build the generator model\n",
    "def build_generator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256 * 16 * 16, activation='relu', input_dim=NOISE_DIM))\n",
    "    model.add(Reshape((16, 16, 256)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128 * 32 * 32, activation='relu'))\n",
    "    model.add(Reshape((32, 32, 128)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(64 * 64 * 64, activation='relu'))\n",
    "    model.add(Reshape((64, 64, 64)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(IMG_CHANNELS, activation='tanh'))\n",
    "    model.add(Reshape((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)))\n",
    "    return model\n",
    "\n",
    "# Build the discriminator model\n",
    "def build_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)))\n",
    "    model.add(Dense(512, activation=LeakyReLU(alpha=0.2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation=LeakyReLU(alpha=0.2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Compile GAN\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Create the combined GAN model\n",
    "discriminator.trainable = False\n",
    "gan_input = Input(shape=(NOISE_DIM,))\n",
    "fake_img = generator(gan_input)\n",
    "gan_output = discriminator(fake_img)\n",
    "gan = Model(gan_input, gan_output)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam())\n",
    "\n",
    "# Function to train GAN\n",
    "def train_gan(epochs, batch_size):\n",
    "    for epoch in range(epochs):\n",
    "        # Train discriminator\n",
    "        idx = np.random.randint(0, images.shape[0], batch_size)\n",
    "        real_imgs = images[idx]\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, NOISE_DIM))\n",
    "        fake_imgs = generator.predict(noise)\n",
    "\n",
    "        d_loss_real = discriminator.train_on_batch(real_imgs, np.ones((batch_size, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch(fake_imgs, np.zeros((batch_size, 1)))\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # Train generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, NOISE_DIM))\n",
    "        g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "        # Print losses and save generated images at intervals\n",
    "        if epoch % SAVE_INTERVAL == 0:\n",
    "            print(f\"{epoch} [D loss: {d_loss[0]:.4f}, acc.: {100 * d_loss[1]:.2f}%] [G loss: {g_loss:.4f}]\")\n",
    "            save_generated_images(epoch)\n",
    "\n",
    "# Function to save generated images\n",
    "def save_generated_images(epoch):\n",
    "    noise = np.random.normal(0, 1, (25, NOISE_DIM))\n",
    "    generated_imgs = generator.predict(noise)\n",
    "    generated_imgs = 0.5 * generated_imgs + 0.5  # Scale back to [0, 1]\n",
    "\n",
    "    fig, axs = plt.subplots(5, 5)\n",
    "    cnt = 0\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            axs[i, j].imshow(generated_imgs[cnt])\n",
    "            axs[i, j].axis('off')\n",
    "            cnt += 1\n",
    "    plt.savefig(f\"gan_generated_epoch_{epoch}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Start training GAN\n",
    "train_gan(EPOCHS, BATCH_SIZE)\n",
    "```\n",
    "\n",
    "### Explanation of the Code\n",
    "\n",
    "1. **Loading and Preprocessing Data**: The `load_data` function loads and resizes images from the specified CelebA dataset directory. The images are normalized to the range `[-1, 1]`.\n",
    "\n",
    "2. **Building the Generator and Discriminator**:\n",
    "   - The generator takes random noise as input and generates images using several dense and reshaping layers, with batch normalization and ReLU activation.\n",
    "   - The discriminator classifies images as real or fake by flattening the input and passing it through several dense layers, using Leaky ReLU and dropout for regularization.\n",
    "\n",
    "3. **Compiling the GAN**: The discriminator is compiled first, and then the combined GAN model is created with the generator and discriminator.\n",
    "\n",
    "4. **Training the GAN**:\n",
    "   - For each epoch, a batch of real images is taken from the dataset, and fake images are generated from random noise.\n",
    "   - The discriminator is trained on both real and fake images.\n",
    "   - The generator is trained to produce images that the discriminator classifies as real.\n",
    "   - The training progress is printed, and generated images are saved at intervals.\n",
    "\n",
    "5. **Saving Generated Images**: The `save_generated_images` function generates and saves images from the generator every few epochs.\n",
    "\n",
    "### Running the Code\n",
    "Make sure to change the `data_path` variable to point to your local CelebA dataset directory. Run the script in a Python environment that supports TensorFlow and Keras. The generated images will be saved periodically in the current working directory.\n",
    "\n",
    "### Applications of GANs\n",
    "Here are some use cases for GANs, as mentioned in your prompt:\n",
    "- **Super-resolution**: Improving the resolution of input images.\n",
    "- **Image inpainting**: Filling in missing parts of images.\n",
    "- **Anime face generation**: Generating images of anime characters.\n",
    "- **Font generation**: Creating new font styles.\n",
    "- **Style transfer**: Applying styles from one image to another.\n",
    "- **Human face generation**: Generating realistic human faces.\n",
    "- **Image to emoji**: Converting images to emoji-like representations.\n",
    "- **Data augmentation**: Creating new training data from existing images.\n",
    "- **Face aging GAN**: Predicting how faces will change over time.\n",
    "- **Photo blending**: Merging two images seamlessly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the implementations for your coding questions related to GANs:\n",
    "\n",
    "### 1. Data Augmentation Function for GAN Training\n",
    "\n",
    "This function generates augmented data for training a GAN using the PIL library for image processing.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "def augment_images(image_dataset, rotation_range=30, flip_prob=0.5, crop_size=(32, 32)):\n",
    "    augmented_images = []\n",
    "    \n",
    "    for img in image_dataset:\n",
    "        # Convert to PIL Image if it's not already\n",
    "        if isinstance(img, np.ndarray):\n",
    "            img = Image.fromarray(img)\n",
    "        \n",
    "        # Random rotation\n",
    "        if rotation_range > 0:\n",
    "            angle = random.uniform(-rotation_range, rotation_range)\n",
    "            img = img.rotate(angle)\n",
    "\n",
    "        # Random horizontal flip\n",
    "        if random.random() < flip_prob:\n",
    "            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        # Random crop\n",
    "        width, height = img.size\n",
    "        crop_x = random.randint(0, width - crop_size[0])\n",
    "        crop_y = random.randint(0, height - crop_size[1])\n",
    "        img = img.crop((crop_x, crop_y, crop_x + crop_size[0], crop_y + crop_size[1]))\n",
    "\n",
    "        augmented_images.append(np.array(img))\n",
    "    \n",
    "    return np.array(augmented_images)\n",
    "\n",
    "# Example usage\n",
    "# augmented_dataset = augment_images(original_dataset)\n",
    "```\n",
    "\n",
    "### 2. Simple Discriminator Model using TensorFlow/Keras\n",
    "\n",
    "Here’s a simple discriminator model that classifies images as real or fake using TensorFlow/Keras.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_discriminator(input_shape=(32, 32, 3)):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=input_shape))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "discriminator = build_discriminator()\n",
    "discriminator.summary()\n",
    "```\n",
    "\n",
    "### 3. Generator Model with Transpose Convolution\n",
    "\n",
    "Here’s the architecture for a generator model that generates 32 x 32 x 3 images from random noise using transpose convolution.\n",
    "\n",
    "```python\n",
    "def build_generator(latent_dim=100):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(latent_dim,)))\n",
    "    \n",
    "    model.add(layers.Dense(128 * 8 * 8, activation='relu'))\n",
    "    model.add(layers.Reshape((8, 8, 128)))\n",
    "    model.add(layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2DTranspose(32, kernel_size=4, strides=2, padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2D(3, kernel_size=7, padding='same', activation='tanh'))  # Output layer for RGB images\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "generator = build_generator()\n",
    "generator.summary()\n",
    "```\n",
    "\n",
    "### 4. Implementing a Minimax Loss Function for GANs\n",
    "\n",
    "This function calculates the Minimax loss for a GAN based on discriminator predictions.\n",
    "\n",
    "```python\n",
    "def minimax_loss(predictions):\n",
    "    # Loss for the generator (G)\n",
    "    g_loss = -tf.reduce_mean(tf.math.log(predictions))\n",
    "    \n",
    "    # Loss for the discriminator (D)\n",
    "    d_loss = -tf.reduce_mean(tf.math.log(1 - predictions))\n",
    "    \n",
    "    return g_loss, d_loss\n",
    "\n",
    "# Example usage\n",
    "# Assuming `discriminator_predictions` is a tensor of shape (batch_size, 1) with the discriminator's outputs\n",
    "# g_loss, d_loss = minimax_loss(discriminator_predictions)\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. **Data Augmentation Function**: Allows you to augment your image dataset with random rotations, flips, and cropping.\n",
    "2. **Discriminator Model**: A simple neural network that takes an image as input and classifies it as real or fake.\n",
    "3. **Generator Model**: A neural network that generates 32x32 RGB images from random noise using transpose convolutions.\n",
    "4. **Minimax Loss Function**: Calculates the Minimax loss for both the generator and discriminator based on their predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
